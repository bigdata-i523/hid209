\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Clustering Algorithms in Big Data Analysis}


\author{Wenxuan Han}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University Bloomington}
  \streetaddress{1150 S Clarizz Blvd}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47401-4294}
}
\email{wenxhan@iu.edu}


% The default list of authors is too long for headers}
% \renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
Data Mining is a kind of popular method which intent to extract and analyze useful information from data. However, the rapid development of computing technologies has caused the size of data sets become extremely large. Because of the high complexity of these data sets, traditional data mining approaches or algorithms are not appropriate to be used. Therefore, it is very important to discover similarities of data and use them to divide data into different groups. Nowadays, clustering algorithms have emerged as a powerful meta-learning tool which provide the goal to categorize data into clusters such that objects are grouped in the same cluster are similar according to specific properties like traits or behaviors. In this paper, we have introduced several types of clustering technologies in data mining with some most commonly used algorithms, and compare advantages and disadvantages between them during the large data sets environment.
\end{abstract}

\keywords{I523, HID209, Big Data, Clustering Algorithms, K-Means, CLIQUE}


\maketitle



\section{Introduction}

Today, human's progress on Internet, Internet of Things (IoT) and other computing technologies lead to the growth of many related applications. Due to the usage of these applications in daily life, there are huge amount of data generated every second which let the concept of big data emerged. Since these data may reveal hidden information which is interesting and important for people to study, the management of big data plays a significant role. We viewed technologies which used to extracting meaningful and required information or to find out unseen relationship between the data as Data Mining \cite{dcar}, and clustering is the one of the core tasks for processing data.

Clustering of big data has gained popularity in the last decade due to increased demand to process of large data sets \cite{bcab}. Data clustering algorithms were developed as a meta-learning tool to analyze massive data accurately. Its main purpose is to find similarities between all objects in a given data set and categorize them into groups by specific metrics which means that clustering is a process to manage similar objects in a same group. And since the data set processed by a clustering algorithm is unlabeled, data clustering has also considered as an unsupervised learning technique \cite{dcaa}.

Because of the large amounts of data produced from different sources in today's world, the demand of efficiency clustering algorithms is increasing. However, it is difficult to create a perfect algorithm which has the capability to satisfy requirement of all situations. Thus, depend on the purposes of clustering and the given data set, people should utilize the corresponding clustering approach \cite{bcab}. Here are the main types of clustering (algorithms) which commonly used:
\begin{itemize}
\item Exclusive or partitioning-based clustering;
\item Hierarchical clustering;
\item Density-based clustering;
\item Grid-based clustering;
\item Model-based clustering.
\end{itemize}
In the rest of this paper we will discuss these cluster types as well as some of the applications.

\section{Clustering Types}
Since each clustering approach has its own idea for solving the specific problems. This section introduces five different clustering types respectively with specific implementation procedures.
\subsection{Exclusive or Partitioning-based Clustering}

Partitioning is a kind of technique which used to decompose the set of data objects into several non-overlapping partitions (each partition represents a cluster) such that each data object is in exactly one partition.

Given a set of $n$ data points, a partitioning method usually start with a random partitioning and refine it iteratively to find a partition of $k$ ($k \leq n$) clusters that optimizes the chosen partitioning criterion. While classifying data points into $k$ groups, it must satisfy two rules: one point must be present in each cluster and one cluster must have one set \cite{dcar}. One of the most famous and commonly used partitioning techniques is k-means.

\subsection{Hierarchical-based Clustering}

Hierarchical clustering groups a big scale of data set by creating a cluster tree or dendrogram. The tree is not a single set of clusters, but rather a multilevel hierarchy, where closeness clusters at one level of leaf nodes are joined as new clusters that compose to nodes at the next level \cite{math}. Generally, there are two approaches applied in hierarchical clustering which are agglomerative method (top-down approach) and divisive method (bottom-up approach).

For agglomerative method, it starts in two or more clusters recursively merged as the most applicable cluster by moving up the hierarchy. However, for divisive method, it starts in a single cluster and recursively split to find applicable cluster for the items by moving down the hierarchy \cite{dcar}. Although the time complexity for both approaches are huge ($O(n^2\log(n))$ for agglomerative and $O(2^n)$ for divisive) which makes them execute too slow during the big data sets, it is possible to choose optimal agglomerative methods such as SLINK for single-linkage and CLINK for complete-linkage clustering to reduce the complexity to $O(n^2)$. BIRCH, CURE, ROCK and Chameleon are some typical algorithms that applied under hierarchical-based clustering.

\subsection{Density-based Clustering}

Density-based clustering identifies distinctive clusters in the data based on the basic idea that a cluster in a data space is a contiguous region of high point density separated by regions of lower object density \cite{jsdbc} and defined as a maximal set of density connected points. It has the capability to learned clusters with arbitrary shape. Techniques such as DBSCAN, OPTICS, DBCLASD and DENCLUE are used to sifter out outliers and determine clusters of arbitrary shape \cite{dcar}.

$\varepsilon-Neighborhood$, which means objects within a radius of $\varepsilon$ from an object, could define the density. It has the following form.
\[N_{\varepsilon}(p):\{q \vert d(p,q)\leq \varepsilon\}\]
Where $q$ is the neighborhood of cluster $p$. A ``high density'' neighborhood of an object contains at least MinPts (a specified number of points) of objects. If a point has more than MinPts objects within $\varepsilon$, it is a core point. If a point has fewer than MinPts objects within $\varepsilon$ but in the neighborhood of a core point, it is a border point. A noise point is neither a core point nor a border point.

\subsection{Grid-based Clustering}

As the name of this clustering technique, it separates data objects in the data sets into different grids. Unlike other clustering methods that concerned with the data points, grid clustering focused on the value space that surrounds the data points. In general, a typical grid-based clustering algorithm has the following five steps:
\begin{itemize}
\item[1] Creating the grid structure. For example, partitioning the data space into a finite number of cells;
\item[2] Calculating the cell density for each cell;
\item[3] Sorting the cells by their densities;
\item[4] Identifying centers of a cluster;
\item[5] Traversal of neighbor cells.
\end{itemize}

Grid-based clustering has the similar time complexity to other clustering methods. However, it could significant reduce the statistical value calculation complexity even though for clustering very big data sets. Some typical examples of this clustering are the Wave-Cluster and STING \cite{dcar}.

\subsection{Model-based Clustering}

Model-based clustering was developed based on probability models from the data. For example, the finite mixture model for probability densities. To find the parameter insider the probability model, it uses maximum likelihood estimation (MLE). In the model-based clustering approach, the data comes from a distribution that is a mixture of two or more components. Each component is described by a density function and has an associated probability in the mixture. Let assume the components adopt $p-variate$ normal distributions. Then the probability model for clustering will often be a mixture of multivariate normal distributions and each component in the mixture will represent a cluster. Nowadays, there are many good techniques that could be used in model-based clustering such as expectation-maximization (EM) algorithm, conceptual clustering and neural network approaches.

\subsection{Comparison of Clustering Methods}

As we have discussed some of the mainly used clustering methods, here we could assess them and sum up their advantages and disadvantages within the big data environment. Table 1 displays these information for each clustering type.
\begin{table*}[htb]
\centering
\begin{tabular}{|p{0.21\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}|} \hline
\textsf{Clustering Type } & \textsf{Advantages} & \textsf{Disadvantages} \\ \hline
Partitioning-based clustering & \begin{itemize}
\item[1] Relaxed to appreciate and implement;
\item[2] Produce additional thick cluster than the hierarchical technique especially when clusters are circular;
\item[3] For large number of variables, k-means algorithm may faster than hierarchical clustering when $k$ is small;
\item[4] Well-organized in processing large data sets.
\end{itemize} & \begin{itemize}
\item[1] Deprived at usage of noisy data and outliers;
\item[2] Works only on numeric data;
\item[3] Unfilled cluster generation problem;
\item[4] Haphazard preliminary cluster center problem;
\item[5] Not appropriate for non-spherical clusters;
\item[6] User has to provide the value of $k$.
\end{itemize} \\ \hline
Hierarchical-based clustering & \begin{itemize}
\item[1] More adaptable;
\item[2] Less delicate to noise and outliers;
\item[3] Any amount of clusters can be acquired by cutting the dendrogram at desired level. It allows diverse users to choose dissimilar panels according to the desired resemblance level;
\item[4] Appropriate to any characteristic type.
\end{itemize} & \begin{itemize}
\item[1] If a process is performed, it cannot be undone;
\item[2] Incompetence to scale well.
\end{itemize} \\ \hline
Density-based clustering & \begin{itemize}
\item[1] Resilient to outliers;
\item[2] Does not necessitate the amount of clusters;
\item[3] Forms clusters of uninformed shapes;
\item[4] Unresponsive to organization of data objects.
\end{itemize} & \begin{itemize}
\item[1] Inappropriate for high-dimensional data sets due to the expletive of dimensionality singularity;
\item[2] Its quality be contingent upon the threshold set.
\end{itemize} \\ \hline 
Grid-based clustering & \begin{itemize}
\item[1] Fast handling time;
\item[2] Self-governing of the amount of data objects.
\end{itemize} & \begin{itemize}
\item[1] Be contingent only on the amount of cells in each dimension in the quantized space.
\end{itemize} \\ \hline
Model-based clustering & \begin{itemize}
\item[1] Vigorous to noisy data or outliers;
\item[2] Fast handling speed;
\item[3] It decides the amount of clusters to produce.
\end{itemize} & \begin{itemize}
\item[1] Multifarious in nature.
\end{itemize} \\ \hline
\end{tabular}
\caption{Comparison between each clustering algorithm \cite{dcar}.}
\end{table*}

\section{Clustering Algorithms}

Here in this section introduces two kinds of well-known and commonly used clustering algorithms: k-means algorithm which applied partitioning-based clustering and CLIQUE algorithm which applied grid-based clustering.

\subsection{K-Means Algorithm}

K-means algorithm is a powerful method to exploring the structure in data set \cite{bdca}. It classifies the whole data set with $n$ instance into $k$ clusters and aims to find an optimal solution that minimizes the value of objective function. K-means uses centroid which is the average points to represent a cluster. And then in assignment step, it obtains the distance between data point and the centroid to find the nearest cluster for each point. After that, update centoids by recomputing them of each cluster. The procedure of k-means algorithm is simple which contains the following main steps:
\begin{itemize}
\item[1] Randomly select $k$ points as initial centroids;
\item[2] Calculate the distance between each data point and centroids;
\item[3] Assign each data point to the centroid with the minimum distance;
\item[4] Repeat the calculation on the centroid of each cluster until centroid does not change.
\end{itemize}

The time complexity of k-means algorithm is $O(nk)$ which could be extremely expensive when both $n$ and $k$ are large. However, there are several ways to overcome this problem such as reduce the cluster number in assignment step or quickly identify data points that often change the cluster. By implementing k-means algorithm through Map-Reduce framework, it will have the ability to handle big data \cite{bdca}.

\subsection{CLIQUE}

CLIQUE, also known as Clustering in QUEst, is used to find density regions from a sparse multi-dimensional data set \cite{bdca}. Each region could be identified as a cluster through properties like attribute values, points or ranges. Since CLIQUE algorithm has the ability to discover density units automatically, it is able to be applied in higher dimensional data sets which is one of its advantages compare to the other clustering algorithms. CLIQUE partitions m-dimensional data space into a rectangular unit without overlapping in order to get the dense units. And clusters are generated from the subspaces of original data spaces through the Apriori property \cite{bdca}. Assume there is a dense unit which is $k$ dimension, its projections are $(k-1)$ dimension. Thus, by using CLIQUE algorithm, we could obtain the minimum descriptions for its data points. CLIQUE algorithm contains the following main steps:
\begin{itemize}
\item[1] Identify subspaces that contain clusters;
\item[2] Identify clusters;
\item[3] Generate the minimum description for the clusters which identified in step 2.
\end{itemize}

\section{Conclusion}

The aim of this paper is to demonstrate the different clustering algorithms used in big data which included partitioning-based clustering, hierarchical-based clustering, density-based clustering, grid-based clustering and model-based clustering, compare their merits and demerits, and extend the content with their applications. Each clustering method adopts a different idea and implementation procedures to processes the data set which characterized by different properties. Thus, for choosing a clustering algorithm, we need to consider about both data sets, time complexity and other constraint conditions. For introducing clustering applications, it gave two simple but widely used algorithm cases: k-means and CLIQUE algorithm. K-means is a kind of partitioning clustering which provides effective results on data sets with numeric attributes, but this could be affected by noise or outliers and its time complexity might be very large in big data set. CLIQUE is a kind of grid clustering, it can find dense regions from multi-dimentional data sets and appropriate to deal with large volume of data. However, it could only apply in numerical data set as well.

\begin{acks}

The author would like to thank Professor Gregor von Laszewski and all TAs for providing the resource, tutorials and other related materials to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

% \input{issues}

\end{document}
