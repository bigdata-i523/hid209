\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Clustering Algorithms in Big Data Analysis}


\author{Wenxuan Han}
% \orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University Bloomington}
  \streetaddress{1150 S Clarizz Blvd}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47401-4294}
}
\email{wenxhan@iu.edu}


% The default list of authors is too long for headers}
% \renewcommand{\shortauthors}{G. v. Laszewski}


\begin{abstract}
Data Mining is a kind of popular method which intent to extract and analyze useful information from data. However, the rapid development of computing technologies has caused the size of data sets become extremely huge. Because of the high complexity of these data sets, traditional data mining approaches or algorithms are not appropriate to be used. Therefore, it is very important to discover the similarities of data and use them to divide data into different groups. Nowadays, clustering algorithms have emerged as a powerful meta-learning tool which provided the goal to categorize data into clusters such that objects are grouped in the same cluster are similar according to specific properties like traits and behaviors. In this paper, we have introduced several types of clustering technologies in data mining with some most commonly used algorithms, and compare the advantages and disadvantages between them during the large data sets environment.
\end{abstract}

\keywords{I523, HID209, Big Data, Clustering Algorithms}


\maketitle



\section{Introduction}

Today, human's progress on Internet, Internet of Things (IoT) and other computing technologies lead to the growth of many related applications. Due to the usage of these applications in human's daily life, there are huge amount of data generated every second which let the concept of big data emerged. Since these data may reveal hidden information which is interesting and important for people to study, the management of big data plays a significant role. We viewed technologies which used to extracting meaningful and required information or to find out unseen relationship between the data as Data Mining \cite{dcar}, and clustering is the one of the main tasks for processing data.

Clustering of big data has gained popularity in the last decade due to increased demand to process of large data sets \cite{bcab}. Data clustering algorithms were developed as a meta-learning tool to analyze massive data accurately. Its main purpose is to find similarities between all objects in a given data set and categorize them into groups by specific metrics which means that clustering is a process to manage similar objects in a same group. And since the data set processed by a clustering algorithm is unlabeled, data clustering has also considered as an unsupervised learning technique \cite{dcaa}.

Because of the large amounts of data produced by many applications from different sources in today's world, the demand of efficiency clustering algorithms is increasing. However, it is difficult to create a perfect algorithm which has the capacity to satisfy requirement of all situations. Thus, depend on the purposes of clustering and the given data set, people should utilize the corresponding clustering approach \cite{bcab}. Here are the main types of clustering (algorithms) which commonly used:
\begin{itemize}
\item Exclusive or partitioning-based clustering;
\item Hierarchical clustering;
\item Density-based clustering;
\item Grid-based clustering;
\item Model-based clustering.
\end{itemize}
In the rest of this paper we will discuss these cluster types as well as some of the applications.

\section{Clustering Types}
In this section, we will see the main par of clustering types. Each clustering has its only purpose to solve the spe problem.
\subsection{Exclusive or Partitioning-based Clustering}

Partitioning is a kind of technique which used to decompose the set of data objects into several non-overlapping partitions (each partition represents a cluster) such that each data object is in exactly one partition.

Given a set of $N$ data points, a partitioning method usually start with a random partitioning and refine it iteratively to find a partition of $K$ ($K \leq N$) clusters that optimizes the chosen partitioning criterion. While classifying data points into $K$ groups, it must satisfy two rules: one point must be present in each cluster and one cluster must have one set \cite{dcar}. One of the most famous and commonly used partitioning techniques is k-means.

\subsection{Hierarchical-based Clustering}

Hierarchical clustering groups a big scale of data set by creating a cluster tree or dendrogram. The tree is not a single set of clusters, but rather a multilevel hierarchy, where closeness clusters at one level of leaf nodes are joined as new clusters that compose to nodes at the next level \cite{math}. Generally, there are two approaches applied in hierarchical clustering which are agglomerative method (top-down approach) and divisive method (bottom-up approach).

For agglomerative method, it starts in two or more clusters recursively merged as the most applicable cluster by moving up the hierarchy. However, for divisive method, it starts in a single cluster and recursively split to find applicable cluster for the items by moving down the hierarchy \cite{dcar}. Although the time complexity for both approaches are huge ($O(n^2\log(n))$ for agglomerative and $O(2^n)$ for divisive) which makes them execute too slow during the big data sets, it is possible to choose optimal agglomerative methods such as SLINK for single-linkage and CLINK for complete-linkage clustering to reduce the complexity to $O(n^2)$. BIRCH, CURE, ROCK and Chameleon are some typical algorithms that applied under hierarchical-based clustering.

\subsection{Density-based Clustering}

Density-based clustering identifies distinctive clusters in the data based on the basic idea that a cluster in a data space is a contiguous region of high point density separated by regions of lower object density \cite{jsdbc} and defined as a maximal set of density connected points. It has the capability to learned clusters with arbitrary shape. Techniques such as DBSCAN, OPTICS, DBCLASD and DENCLUE are used to sifter out outliers and determine clusters of arbitrary shape \cite{dcar}.

$\varepsilon-Neighborhood$ which means objects within a radius of $\varepsilon$ from an object could define the density. It has the following form.
\[N_{\varepsilon}(p):\{q \vert d(p,q)\leq \varepsilon\}\]
Where $q$ is the neighborhood of cluster $p$. A ``high density'' neighborhood of an object contains at least MinPts (a specified number of points) of objects. If a point has more than MinPts objects within $\varepsilon$, it is a core point. If a point has fewer than MinPts objects within $\varepsilon$ but in the neighborhood of a core point, it is a border point. A noise point is neither a core point nor a border point.

\subsection{Grid-based Clustering}

As the name of this clustering technique, it separates data objects in the data sets into different grids. Unlike other clustering methods that concerned with the data points, grid clustering focused on the value space that surrounds the data points. In general, a typical grid-based clustering algorithm has the following five steps:
\begin{itemize}
\item[1] Creating the grid structure. For example, partitioning the data space into a finite number of cells.
\item[2] Calculating the cell density for each cell.
\item[3] Sorting the cells by their densities.
\item[4] Identifying centers of a cluster.
\item[5] Traversal of neighbor cells.
\end{itemize}

Grid-based clustering has the similar time complexity to other clustering methods. However, it could significant reduce the statistical value calculation complexity even though for clustering very big data sets. Some typical examples of this clustering are the Wave-Cluster and STING \cite{dcar}.

\subsection{Model-based Clustering}

Model-based clustering was developed based on probability models from the data. For example, the finite mixture model for probability densities. To find the parameter insider the probability model, it uses maximum likelihood estimation (MLE). In the model-based clustering approach, the data comes from a distribution that is a mixture of two or more components. Each component is described by a density function and has an associated probability in the mixture. Let assume the components adopt $p-variate$ normal distributions. Then the probability model for clustering will often be a mixture of multivariate normal distributions and each component in the mixture will represent a cluster. Nowadays, there are many good techniques that could be used in model-based clustering such as expectation-maximization (EM) algorithm, conceptual clustering and neural network approaches.

\subsection{Comparson}

\section{Clustering Algorithms}

Here in this section introduces two kinds of well-known and commonly used clustering algorithms: k-means algorithm which applied partitioning-based clustering and CLIQUE algorithm which applied grid-based clustering.

\subsection{K-means Algorithm}



\subsection{CLIQUE}

\section{Conclusion}


\begin{acks}

The author would like to thank Professor Gregor von Laszewski and all TAs for providing the resource, tutorials and other related materials to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

% \input{issues}

\end{document}
